{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ecrGsYFSHOh"
      },
      "source": [
        "apostrophe_dict = {\n",
        "\"ain't\": \"am not / are not\",\n",
        "\"aren't\": \"are not / am not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he had / he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he shall / he will\",\n",
        "\"he'll've\": \"he shall have / he will have\",\n",
        "\"he's\": \"he has / he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how has / how is\",\n",
        "\"i'd\": \"I had / I would\",\n",
        "\"i'd've\": \"I would have\",\n",
        "\"i'll\": \"I shall / I will\",\n",
        "\"i'll've\": \"I shall have / I will have\",\n",
        "\"i'm\": \"I am\",\n",
        "\"i've\": \"I have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it had / it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it shall / it will\",\n",
        "\"it'll've\": \"it shall have / it will have\",\n",
        "\"it's\": \"it has / it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she had / she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she shall / she will\",\n",
        "\"she'll've\": \"she shall have / she will have\",\n",
        "\"she's\": \"she has / she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so as / so is\",\n",
        "\"that'd\": \"that would / that had\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that has / that is\",\n",
        "\"there'd\": \"there had / there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there has / there is\",\n",
        "\"they'd\": \"they had / they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they shall / they will\",\n",
        "\"they'll've\": \"they shall have / they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we had / we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what shall / what will\",\n",
        "\"what'll've\": \"what shall have / what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what has / what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when has / when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where has / where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who shall / who will\",\n",
        "\"who'll've\": \"who shall have / who will have\",\n",
        "\"who's\": \"who has / who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why has / why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you had / you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you shall / you will\",\n",
        "\"you'll've\": \"you shall have / you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "short_word_dict = {\n",
        "\"121\": \"one to one\",\n",
        "\"a/s/l\": \"age, sex, location\",\n",
        "\"adn\": \"any day now\",\n",
        "\"afaik\": \"as far as I know\",\n",
        "\"afk\": \"away from keyboard\",\n",
        "\"aight\": \"alright\",\n",
        "\"alol\": \"actually laughing out loud\",\n",
        "\"b4\": \"before\",\n",
        "\"b4n\": \"bye for now\",\n",
        "\"bak\": \"back at the keyboard\",\n",
        "\"bf\": \"boyfriend\",\n",
        "\"bff\": \"best friends forever\",\n",
        "\"bfn\": \"bye for now\",\n",
        "\"bg\": \"big grin\",\n",
        "\"bta\": \"but then again\",\n",
        "\"btw\": \"by the way\",\n",
        "\"cid\": \"crying in disgrace\",\n",
        "\"cnp\": \"continued in my next post\",\n",
        "\"cp\": \"chat post\",\n",
        "\"cu\": \"see you\",\n",
        "\"cul\": \"see you later\",\n",
        "\"cul8r\": \"see you later\",\n",
        "\"cya\": \"bye\",\n",
        "\"cyo\": \"see you online\",\n",
        "\"dbau\": \"doing business as usual\",\n",
        "\"fud\": \"fear, uncertainty, and doubt\",\n",
        "\"fwiw\": \"for what it's worth\",\n",
        "\"fyi\": \"for your information\",\n",
        "\"g\": \"grin\",\n",
        "\"g2g\": \"got to go\",\n",
        "\"ga\": \"go ahead\",\n",
        "\"gal\": \"get a life\",\n",
        "\"gf\": \"girlfriend\",\n",
        "\"gfn\": \"gone for now\",\n",
        "\"gmbo\": \"giggling my butt off\",\n",
        "\"gmta\": \"great minds think alike\",\n",
        "\"h8\": \"hate\",\n",
        "\"hagn\": \"have a good night\",\n",
        "\"hdop\": \"help delete online predators\",\n",
        "\"hhis\": \"hanging head in shame\",\n",
        "\"iac\": \"in any case\",\n",
        "\"ianal\": \"I am not a lawyer\",\n",
        "\"ic\": \"I see\",\n",
        "\"idk\": \"I don't know\",\n",
        "\"imao\": \"in my arrogant opinion\",\n",
        "\"imnsho\": \"in my not so humble opinion\",\n",
        "\"imo\": \"in my opinion\",\n",
        "\"iow\": \"in other words\",\n",
        "\"ipn\": \"I’m posting naked\",\n",
        "\"irl\": \"in real life\",\n",
        "\"jk\": \"just kidding\",\n",
        "\"l8r\": \"later\",\n",
        "\"ld\": \"later, dude\",\n",
        "\"ldr\": \"long distance relationship\",\n",
        "\"llta\": \"lots and lots of thunderous applause\",\n",
        "\"lmao\": \"laugh my ass off\",\n",
        "\"lmirl\": \"let's meet in real life\",\n",
        "\"lol\": \"laugh out loud\",\n",
        "\"ltr\": \"longterm relationship\",\n",
        "\"lulab\": \"love you like a brother\",\n",
        "\"lulas\": \"love you like a sister\",\n",
        "\"luv\": \"love\",\n",
        "\"m/f\": \"male or female\",\n",
        "\"m8\": \"mate\",\n",
        "\"milf\": \"mother I would like to fuck\",\n",
        "\"oll\": \"online love\",\n",
        "\"omg\": \"oh my god\",\n",
        "\"otoh\": \"on the other hand\",\n",
        "\"pir\": \"parent in room\",\n",
        "\"ppl\": \"people\",\n",
        "\"r\": \"are\",\n",
        "\"rofl\": \"roll on the floor laughing\",\n",
        "\"rpg\": \"role playing games\",\n",
        "\"ru\": \"are you\",\n",
        "\"shid\": \"slaps head in disgust\",\n",
        "\"somy\": \"sick of me yet\",\n",
        "\"sot\": \"short of time\",\n",
        "\"thanx\": \"thanks\",\n",
        "\"thx\": \"thanks\",\n",
        "\"ttyl\": \"talk to you later\",\n",
        "\"u\": \"you\",\n",
        "\"ur\": \"you are\",\n",
        "\"uw\": \"you’re welcome\",\n",
        "\"wb\": \"welcome back\",\n",
        "\"wfm\": \"works for me\",\n",
        "\"wibni\": \"wouldn't it be nice if\",\n",
        "\"wtf\": \"what the fuck\",\n",
        "\"wtg\": \"way to go\",\n",
        "\"wtgp\": \"want to go private\",\n",
        "\"ym\": \"young man\",\n",
        "\"gr8\": \"great\"\n",
        "}\n",
        "\n",
        "\n",
        "emoticon_dict = {\n",
        "\":)\": \"happy\",\n",
        "\":‑)\": \"happy\",\n",
        "\":-]\": \"happy\",\n",
        "\":-3\": \"happy\",\n",
        "\":->\": \"happy\",\n",
        "\"8-)\": \"happy\",\n",
        "\":-}\": \"happy\",\n",
        "\":o)\": \"happy\",\n",
        "\":c)\": \"happy\",\n",
        "\":^)\": \"happy\",\n",
        "\"=]\": \"happy\",\n",
        "\"=)\": \"happy\",\n",
        "\"<3\": \"happy\",\n",
        "\":-(\": \"sad\",\n",
        "\":(\": \"sad\",\n",
        "\":c\": \"sad\",\n",
        "\":<\": \"sad\",\n",
        "\":[\": \"sad\",\n",
        "\">:[\": \"sad\",\n",
        "\":{\": \"sad\",\n",
        "\">:(\": \"sad\",\n",
        "\":-c\": \"sad\",\n",
        "\":-< \": \"sad\",\n",
        "\":-[\": \"sad\",\n",
        "\":-||\": \"sad\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QHQOC6X3_-b"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kKAaQWm_pp8"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRWU-rLS2T5p",
        "outputId": "00c809be-1499-4575-e3fb-5db6ce52959c"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import tokenize as tknz\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoNFR3fO2cUn"
      },
      "source": [
        "#pd.set_option('display.max_columns', None)  \n",
        "#pd.set_option('display.expand_frame_repr', False)\n",
        "pd.set_option('max_colwidth', 800)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "an4_K-ZnHpLu"
      },
      "source": [
        "class TextPreprocessor:\n",
        "    def __init__(self, file, main_inf, csv_path = True):\n",
        "        '''\n",
        "        file - file path if csv_path true else pandas file\n",
        "        main_inf - base column name\n",
        "        '''\n",
        "        self.start_name = main_inf\n",
        "        self.last_col = None\n",
        "        if csv_path:\n",
        "            self.data = pd.read_csv(file)\n",
        "        else:\n",
        "            self.data = file.copy()\n",
        "        self.data_work = self.data.copy()\n",
        "\n",
        "    def get_data(self):\n",
        "        return self.data.copy()\n",
        "\n",
        "    def get_data_work(self):\n",
        "        return self.data_work.copy()\n",
        "\n",
        "    def get_start_name(self):\n",
        "        return self.start_name\n",
        "\n",
        "    def clean_users(self):\n",
        "        regex = re.compile(\"@[\\w]*\")\n",
        "\n",
        "        def without_mention(text, regex=regex):\n",
        "            return re.sub(regex, ' ', text)\n",
        "        \n",
        "        work_col = self.start_name+'_wo_mention'\n",
        "\n",
        "        self.data_work[work_col] = None\n",
        "        for i in range(0, self.data_work.shape[0]):\n",
        "            self.data_work[work_col][i] = without_mention(self.data_work[self.start_name][i]).lower()\n",
        "\n",
        "        self.last_col = work_col\n",
        "\n",
        "    def change_words_by_dict(self, dict_to_check, dict_name):\n",
        "        if self.last_col:\n",
        "            work_col = self.start_name+'_wo_dist_'+dict_name\n",
        "            where_col = self.last_col\n",
        "        else:\n",
        "            work_col = self.start_name+'_wo_dist_'+dict_name\n",
        "            where_col = self.start_name\n",
        "        \n",
        "        self.data_work[work_col] = None\n",
        "\n",
        "        for i in range(0, self.data_work.shape[0]):\n",
        "            text = self.data_work[where_col][i] \n",
        "            for word in text.split():\n",
        "                if word in dict_to_check:\n",
        "                    pos = text.find(word)\n",
        "                    text = text[:pos] + dict_to_check[word] +' ' + text[pos+len(word)+1:]\n",
        "            self.data_work[work_col][i] = text\n",
        "        \n",
        "        self.last_col = work_col\n",
        "    \n",
        "    def delete_punct(self): \n",
        "        regex2 = re.compile( r'[^\\w\\s]')\n",
        "\n",
        "        def wo_punct(text, regex=regex2):\n",
        "            return re.sub(regex, ' ', text)\n",
        "\n",
        "        if self.last_col:\n",
        "            work_col = self.start_name+'_wo_dist_punct'\n",
        "            where_col = self.last_col\n",
        "        else:\n",
        "            work_col = self.start_name+'_wo_dist_punct'\n",
        "            where_col = self.start_name\n",
        "\n",
        "        self.data_work[work_col] = None\n",
        "        for i in range(0, self.data_work.shape[0]):\n",
        "            self.data_work[work_col][i] = wo_punct(self.data_work[where_col][i]).lower()\n",
        "\n",
        "        self.last_col = work_col\n",
        "\n",
        "\n",
        "    def delete_spec(self):\n",
        "        regex3 = re.compile( r'[^a-zA-Z0-9]')\n",
        "\n",
        "        def wo_spec(text, regex=regex3):\n",
        "            return re.sub(regex, ' ', text)\n",
        "\n",
        "        if self.last_col:\n",
        "            work_col = self.start_name+'_wo_dist_spec'\n",
        "            where_col = self.last_col\n",
        "        else:\n",
        "            work_col = self.start_name+'_wo_dist_spec'\n",
        "            where_col = self.start_name\n",
        "\n",
        "        self.data_work[work_col] = None\n",
        "        for i in range(0, self.data_work.shape[0]):\n",
        "            self.data_work[work_col][i] = wo_spec(self.data_work[where_col][i]).lower()\n",
        "\n",
        "        self.last_col = work_col       \n",
        "        \n",
        "\n",
        "    def delete_numb(self): \n",
        "        regex4 = re.compile( r'[^a-zA-Z]')\n",
        "\n",
        "        def wo_numb(text, regex=regex4):\n",
        "            return re.sub(regex, ' ', text)\n",
        "\n",
        "        if self.last_col:\n",
        "            work_col = self.start_name+'_wo_dist_numb'\n",
        "            where_col = self.last_col\n",
        "        else:\n",
        "            work_col = self.start_name+'_wo_dist_numb'\n",
        "            where_col = self.start_name\n",
        "\n",
        "        self.data_work[work_col] = None\n",
        "        for i in range(0, self.data_work.shape[0]):\n",
        "            self.data_work[work_col][i] = wo_numb(self.data_work[where_col][i]).lower()\n",
        "\n",
        "        self.last_col = work_col\n",
        "\n",
        "    def delete_short_words(self, length=1):\n",
        "\n",
        "        if self.last_col:\n",
        "            work_col = self.start_name+'_wo_short_words'\n",
        "            where_col = self.last_col\n",
        "        else:\n",
        "            work_col = self.start_name+'_wo_short_words'\n",
        "            where_col = self.start_name\n",
        "\n",
        "        self.data_work[work_col] = None\n",
        "        for i in range(0, self.data_work.shape[0]):\n",
        "            text = self.data_work[where_col][i] \n",
        "            self.data_work[work_col][i] = ' '.join([w for w in text.split() if len(w)>length])\n",
        "        \n",
        "        self.last_col = work_col\n",
        "\n",
        "    def do_tokinze(self):\n",
        "\n",
        "        if self.last_col:\n",
        "            work_col = self.start_name+'_tokens'\n",
        "            where_col = self.last_col\n",
        "        else:\n",
        "            work_col = self.start_name+'_tokens'\n",
        "            where_col = self.start_name\n",
        "\n",
        "        self.data_work[work_col] = None\n",
        "        for i in range(0, self.data_work.shape[0]):\n",
        "            text = self.data_work[where_col][i] \n",
        "            self.data_work[work_col][i] = tknz.word_tokenize(text)\n",
        "\n",
        "        self.last_col = work_col\n",
        "\n",
        "    \n",
        "    def wo_stopwords(self):\n",
        "\n",
        "        if self.last_col:\n",
        "            work_col = self.start_name+'_tokens_wo_stopwords'\n",
        "            where_col = self.last_col\n",
        "        else:\n",
        "            work_col = self.start_name+'_tokens_wo_stopwords'\n",
        "            where_col = self.start_name\n",
        "\n",
        "        stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "        self.data_work[work_col] = None\n",
        "        for i in range(0, self.data_work.shape[0]):\n",
        "            li = self.data_work[where_col][i] \n",
        "            self.data_work[work_col][i] = [word for word in li if not word in stop_words]\n",
        "\n",
        "        self.last_col = work_col\n",
        "\n",
        "\n",
        "    def stemmer(self):\n",
        "        if self.last_col:\n",
        "            work_col = self.start_name+'_stemm'\n",
        "            where_col = self.last_col\n",
        "        else:\n",
        "            work_col = self.start_name+'_stemm'\n",
        "            where_col = self.start_name\n",
        "\n",
        "        stemmer = PorterStemmer()\n",
        "        self.data_work[work_col] = None\n",
        "        \n",
        "        for i in range(0, self.data_work.shape[0]):\n",
        "            li = self.data_work[where_col][i]\n",
        "            res = [] \n",
        "            for word in li:\n",
        "                res.append(stemmer.stem(word))\n",
        "            self.data_work[work_col][i] = res\n",
        "      \n",
        "        self.last_col = work_col\n",
        "\n",
        "    def lemmer(self):\n",
        "        if self.last_col:\n",
        "            work_col = self.start_name+'_lemm'\n",
        "            where_col = self.last_col\n",
        "        else:\n",
        "            work_col = self.start_name+'_lemm'\n",
        "            where_col = self.start_name\n",
        "\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        self.data_work[work_col] = None\n",
        "\n",
        "        for i in range(0, self.data_work.shape[0]):\n",
        "            li = self.data_work[where_col][i]\n",
        "            res = [] \n",
        "            for word in li:\n",
        "                res.append(lemmatizer.lemmatize(word))\n",
        "            self.data_work[work_col][i] = ' '.join(res)\n",
        "        \n",
        "        self.last_col = work_col\n",
        "    \n",
        "    def add_final_inf_to_data(self):\n",
        "        self.data['Preproc_text'] = self.data_work[self.last_col]\n",
        "\n",
        "    def full_preprocessor(self):\n",
        "        self.clean_users()\n",
        "        self.change_words_by_dict(apostrophe_dict, 'apostr')\n",
        "        self.change_words_by_dict(short_word_dict, 'shw')\n",
        "        self.change_words_by_dict(emoticon_dict, 'emot')\n",
        "        self.delete_punct()\n",
        "        self.delete_spec()\n",
        "        self.delete_numb()\n",
        "        self.delete_short_words()\n",
        "        self.do_tokinze()\n",
        "        self.wo_stopwords()\n",
        "        self.stemmer()\n",
        "        self.lemmer()\n",
        "        self.add_final_inf_to_data()\n",
        "      \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNXPnnWuNON0"
      },
      "source": [
        "TP = TextPreprocessor('train_tweets.csv', 'tweet')\n",
        "TP.full_preprocessor()\n",
        "TP.get_data_work()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baVCoZ997BJs"
      },
      "source": [
        "TP.get_data().to_pickle(path='tweet_prepared.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}